{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3422664,
          "sourceType": "datasetVersion",
          "datasetId": 1814312
        },
        {
          "sourceId": 14346076,
          "sourceType": "datasetVersion",
          "datasetId": 9160048
        }
      ],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "oct-GAN",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "mk-OO5B2VRhC"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "mathieugodbout_oct_postsurgery_visual_improvement_path = kagglehub.dataset_download('mathieugodbout/oct-postsurgery-visual-improvement')\n",
        "muhammadusamazaman_sample_path = kagglehub.dataset_download('muhammadusamazaman/sample')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "WAdcxOIkVRhF"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "KK8PN1qqVRhG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "yPDn4zamVRhH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:26.357843Z",
          "iopub.execute_input": "2025-12-30T17:03:26.358655Z",
          "iopub.status.idle": "2025-12-30T17:03:26.362108Z",
          "shell.execute_reply.started": "2025-12-30T17:03:26.358625Z",
          "shell.execute_reply": "2025-12-30T17:03:26.36153Z"
        },
        "id": "lr78tHReVRhJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df =pd.read_csv(\"/kaggle/input/oct-postsurgery-visual-improvement/oct_data/train/clinical_data.csv\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:26.573398Z",
          "iopub.execute_input": "2025-12-30T17:03:26.573619Z",
          "iopub.status.idle": "2025-12-30T17:03:26.586799Z",
          "shell.execute_reply.started": "2025-12-30T17:03:26.573602Z",
          "shell.execute_reply": "2025-12-30T17:03:26.586229Z"
        },
        "id": "Oig692bTVRhK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:26.720597Z",
          "iopub.execute_input": "2025-12-30T17:03:26.721228Z",
          "iopub.status.idle": "2025-12-30T17:03:26.733771Z",
          "shell.execute_reply.started": "2025-12-30T17:03:26.721209Z",
          "shell.execute_reply": "2025-12-30T17:03:26.733213Z"
        },
        "id": "zsO7a6BKVRhL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "df.describe()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:26.890471Z",
          "iopub.execute_input": "2025-12-30T17:03:26.890987Z",
          "iopub.status.idle": "2025-12-30T17:03:26.924802Z",
          "shell.execute_reply.started": "2025-12-30T17:03:26.89097Z",
          "shell.execute_reply": "2025-12-30T17:03:26.924233Z"
        },
        "id": "j3uLwKZKVRhN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:27.075405Z",
          "iopub.execute_input": "2025-12-30T17:03:27.075665Z",
          "iopub.status.idle": "2025-12-30T17:03:27.080277Z",
          "shell.execute_reply.started": "2025-12-30T17:03:27.075644Z",
          "shell.execute_reply": "2025-12-30T17:03:27.079642Z"
        },
        "id": "V-3njQwDVRhO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:27.279679Z",
          "iopub.execute_input": "2025-12-30T17:03:27.279946Z",
          "iopub.status.idle": "2025-12-30T17:03:27.286097Z",
          "shell.execute_reply.started": "2025-12-30T17:03:27.279926Z",
          "shell.execute_reply": "2025-12-30T17:03:27.285587Z"
        },
        "id": "mGR0OdbeVRhP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df[\"age\"], kde=True)\n",
        "plt.title(\"Age Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:27.475714Z",
          "iopub.execute_input": "2025-12-30T17:03:27.476592Z",
          "iopub.status.idle": "2025-12-30T17:03:27.702526Z",
          "shell.execute_reply.started": "2025-12-30T17:03:27.476549Z",
          "shell.execute_reply": "2025-12-30T17:03:27.70168Z"
        },
        "id": "Zy1YyoYrVRhQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df[\"mh_size\"], kde=True)\n",
        "plt.title(\"Macular Hole Size Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:30.210854Z",
          "iopub.execute_input": "2025-12-30T17:03:30.211147Z",
          "iopub.status.idle": "2025-12-30T17:03:30.424292Z",
          "shell.execute_reply.started": "2025-12-30T17:03:30.211125Z",
          "shell.execute_reply": "2025-12-30T17:03:30.423585Z"
        },
        "id": "pfEhEyDkVRhR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:30.998313Z",
          "iopub.execute_input": "2025-12-30T17:03:30.998952Z",
          "iopub.status.idle": "2025-12-30T17:03:31.70578Z",
          "shell.execute_reply.started": "2025-12-30T17:03:30.998929Z",
          "shell.execute_reply": "2025-12-30T17:03:31.705102Z"
        },
        "id": "Dje7V0BfVRhR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for column in ['age', 'mh_duration', 'mh_size', 'VA_baseline', 'VA_2weeks', 'VA_3months', 'VA_6months', 'VA_12months']:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.histplot(df[column], kde=True)\n",
        "    plt.title(f'Distribution of {column}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:35.990312Z",
          "iopub.execute_input": "2025-12-30T17:03:35.990876Z",
          "iopub.status.idle": "2025-12-30T17:03:37.659169Z",
          "shell.execute_reply.started": "2025-12-30T17:03:35.990849Z",
          "shell.execute_reply": "2025-12-30T17:03:37.658387Z"
        },
        "id": "xHvLOXTBVRhS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install tifffile"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:43.038016Z",
          "iopub.execute_input": "2025-12-30T17:03:43.038717Z",
          "iopub.status.idle": "2025-12-30T17:03:46.159628Z",
          "shell.execute_reply.started": "2025-12-30T17:03:43.038692Z",
          "shell.execute_reply": "2025-12-30T17:03:46.158772Z"
        },
        "id": "kjY6In3sVRhT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Define the Encoders (Fundus & OCT)**"
      ],
      "metadata": {
        "id": "TNoFac8eVRhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os, glob\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import tifffile as tiff\n",
        "# from torchvision.utils import save_image\n",
        "\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# print(\"DEVICE:\", DEVICE)\n",
        "\n",
        "# DATASET_ROOT = \"/kaggle/input/oct-postsurgery-visual-improvement\"  # adjust if Kaggle shows slightly different name\n",
        "# TRAIN_DIR = os.path.join(DATASET_ROOT, \"oct_data\", \"train\", \"octs\")\n",
        "# VAL_DIR   = os.path.join(DATASET_ROOT, \"oct_data\", \"val\", \"octs\")\n",
        "# TEST_DIR  = os.path.join(DATASET_ROOT, \"oct_data\", \"test\", \"octs\")\n",
        "\n",
        "# META_PATH = os.path.join(DATASET_ROOT, \"oct_data\",\"train\" ,\"clinical_data.csv\")  # should exist in Kaggle dataset\n",
        "\n",
        "# print(\"Train dir exists:\", os.path.exists(TRAIN_DIR))\n",
        "\n",
        "# print(\"Meta exists:\", os.path.exists(META_PATH))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:46.16125Z",
          "iopub.execute_input": "2025-12-30T17:03:46.161486Z",
          "iopub.status.idle": "2025-12-30T17:03:46.165352Z",
          "shell.execute_reply.started": "2025-12-30T17:03:46.161462Z",
          "shell.execute_reply": "2025-12-30T17:03:46.164738Z"
        },
        "id": "lIbT0WrHVRhV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 1: Setup + Paths\n",
        "# =========================\n",
        "!pip -q install tifffile\n",
        "\n",
        "import os, glob, re, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tifffile as tiff\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"DEVICE:\", DEVICE)\n",
        "\n",
        "# Kaggle dataset root (as in your screenshot)\n",
        "DATASET_ROOT = \"/kaggle/input/oct-postsurgery-visual-improvement\"\n",
        "TRAIN_DIR = os.path.join(DATASET_ROOT, \"oct_data\", \"train\", \"octs\")\n",
        "VAL_DIR   = os.path.join(DATASET_ROOT, \"oct_data\", \"val\", \"octs\")\n",
        "TEST_DIR  = os.path.join(DATASET_ROOT, \"oct_data\", \"test\", \"octs\")\n",
        "\n",
        "META_PATH = os.path.join(DATASET_ROOT, \"clinical_data.csv\")  # FIXED\n",
        "\n",
        "print(\"Train:\", os.path.exists(TRAIN_DIR), TRAIN_DIR)\n",
        "print(\"Val  :\", os.path.exists(VAL_DIR), VAL_DIR)\n",
        "print(\"Meta :\", os.path.exists(META_PATH), META_PATH)\n",
        "\n",
        "# Hyperparams\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 20\n",
        "LR_G = 2e-4\n",
        "LR_D = 2e-4\n",
        "Z_DIM = 128\n",
        "\n",
        "LAMBDA_L1   = 10.0\n",
        "LAMBDA_SSIM = 2.0\n",
        "LAMBDA_FREQ = 1.0\n",
        "\n",
        "OUT_DIR = \"/kaggle/working/hmdf_outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:46.166211Z",
          "iopub.execute_input": "2025-12-30T17:03:46.166464Z",
          "iopub.status.idle": "2025-12-30T17:03:49.352766Z",
          "shell.execute_reply.started": "2025-12-30T17:03:46.166443Z",
          "shell.execute_reply": "2025-12-30T17:03:49.351892Z"
        },
        "id": "49NWd7cRVRhV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 2: Losses + FFT + SSIM\n",
        "# =========================\n",
        "def hinge_d_loss(d_real, d_fake):\n",
        "    return (F.relu(1.0 - d_real).mean() + F.relu(1.0 + d_fake).mean())\n",
        "\n",
        "def hinge_g_loss(d_fake):\n",
        "    return (-d_fake).mean()\n",
        "\n",
        "def fft_mag(x: torch.Tensor) -> torch.Tensor:\n",
        "    X = torch.fft.fft2(x, norm=\"ortho\")\n",
        "    return torch.sqrt(X.real**2 + X.imag**2 + 1e-8)\n",
        "\n",
        "def ssim_loss(x: torch.Tensor, y: torch.Tensor, C1=0.01**2, C2=0.03**2) -> torch.Tensor:\n",
        "    mu_x = F.avg_pool2d(x, 3, 1, 1)\n",
        "    mu_y = F.avg_pool2d(y, 3, 1, 1)\n",
        "    sigma_x = F.avg_pool2d(x * x, 3, 1, 1) - mu_x * mu_x\n",
        "    sigma_y = F.avg_pool2d(y * y, 3, 1, 1) - mu_y * mu_y\n",
        "    sigma_xy = F.avg_pool2d(x * y, 3, 1, 1) - mu_x * mu_y\n",
        "\n",
        "    ssim_map = ((2 * mu_x * mu_y + C1) * (2 * sigma_xy + C2)) / (\n",
        "        (mu_x * mu_x + mu_y * mu_y + C1) * (sigma_x + sigma_y + C2) + 1e-8\n",
        "    )\n",
        "    return 1.0 - ssim_map.mean()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:53.660853Z",
          "iopub.execute_input": "2025-12-30T17:03:53.661156Z",
          "iopub.status.idle": "2025-12-30T17:03:53.66834Z",
          "shell.execute_reply.started": "2025-12-30T17:03:53.661126Z",
          "shell.execute_reply": "2025-12-30T17:03:53.667504Z"
        },
        "id": "94EiWypqVRhV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imagecodecs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:03:57.852882Z",
          "iopub.execute_input": "2025-12-30T17:03:57.853288Z",
          "iopub.status.idle": "2025-12-30T17:04:00.98973Z",
          "shell.execute_reply.started": "2025-12-30T17:03:57.853258Z",
          "shell.execute_reply": "2025-12-30T17:04:00.988952Z"
        },
        "id": "M5-dxcqHVRhW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 3: Dataset (TIFF) + Metadata + Timepoints (FIXED)\n",
        "# =========================\n",
        "TIMEPOINTS = [\"baseline\", \"2weeks\", \"3months\", \"6months\", \"12months\"]\n",
        "\n",
        "def parse_filename(path: str):\n",
        "    # Examples:\n",
        "    # 104_baseline_H.tiff\n",
        "    # 104_6months_V.tiff\n",
        "    base = os.path.basename(path)\n",
        "    name = os.path.splitext(base)[0]\n",
        "    parts = name.split(\"_\")\n",
        "    pid = int(parts[0])\n",
        "    tp = parts[1].lower() if len(parts) > 1 else \"baseline\"\n",
        "    orient = parts[2].upper() if len(parts) > 2 else \"H\"\n",
        "    if tp not in TIMEPOINTS:\n",
        "        # fallback: try to detect\n",
        "        for t in TIMEPOINTS:\n",
        "            if t in name.lower():\n",
        "                tp = t\n",
        "                break\n",
        "    return pid, tp, orient\n",
        "\n",
        "class OCTDataset(Dataset):\n",
        "    def __init__(self, folder, meta_path=None, img_size=256, use_meta=True):\n",
        "        self.files = sorted(glob.glob(os.path.join(folder, \"*.tiff\")))\n",
        "        self.img_size = img_size\n",
        "        self.use_meta = use_meta and (meta_path is not None) and os.path.exists(meta_path)\n",
        "\n",
        "        self.meta = None\n",
        "        self.cond_cols = [\"age\",\"sex\",\"pseudophakic\",\"mh_duration\",\"elevated_edge\",\"mh_size\",\"VA_baseline\"]\n",
        "\n",
        "        if self.use_meta:\n",
        "            self.meta = pd.read_csv(meta_path).set_index(\"id\")\n",
        "            self.cond_cols = [c for c in self.cond_cols if c in self.meta.columns]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def _read_tiff_norm(self, path):\n",
        "        arr = tiff.imread(path)\n",
        "        if arr.ndim == 3:\n",
        "            arr = arr[..., 0]\n",
        "        arr = arr.astype(np.float32)\n",
        "        arr = (arr - arr.min()) / ((arr.max() - arr.min()) + 1e-8)  # [0,1]\n",
        "        x = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0)          # (1,1,H,W)\n",
        "        x = F.interpolate(x, size=(self.img_size, self.img_size), mode=\"bilinear\", align_corners=False)\n",
        "        x = x.squeeze(0)                                             # (1,H,W)\n",
        "        x = x * 2.0 - 1.0                                            # [-1,1]\n",
        "        return x\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.files[idx]\n",
        "        pid, tp, orient = parse_filename(path)\n",
        "        img = self._read_tiff_norm(path)\n",
        "\n",
        "        # timepoint one-hot\n",
        "        tp_vec = torch.zeros(len(TIMEPOINTS), dtype=torch.float32)\n",
        "        if tp in TIMEPOINTS:\n",
        "            tp_vec[TIMEPOINTS.index(tp)] = 1.0\n",
        "\n",
        "        if self.use_meta and (pid in self.meta.index):\n",
        "            row = self.meta.loc[pid]\n",
        "            vals = []\n",
        "            for c in self.cond_cols:\n",
        "                v = row[c]\n",
        "                if pd.isna(v): v = 0.0\n",
        "                vals.append(float(v))\n",
        "            cond = torch.tensor(vals, dtype=torch.float32)\n",
        "            # stable standardization\n",
        "            cond = (cond - cond.mean()) / (cond.std() + 1e-6)\n",
        "            cond = torch.cat([cond, tp_vec], dim=0)\n",
        "        else:\n",
        "            cond = tp_vec\n",
        "\n",
        "        return {\"img\": img, \"cond\": cond, \"pid\": pid, \"tp\": tp, \"path\": path}\n",
        "\n",
        "ds_train = OCTDataset(TRAIN_DIR, META_PATH, img_size=IMG_SIZE, use_meta=True)\n",
        "ds_val   = OCTDataset(VAL_DIR, META_PATH, img_size=IMG_SIZE, use_meta=True)\n",
        "\n",
        "print(\"Train samples:\", len(ds_train))\n",
        "print(\"Val samples  :\", len(ds_val))\n",
        "s = ds_train[0]\n",
        "print(\"Example:\", s[\"pid\"], s[\"tp\"], s[\"img\"].shape, s[\"cond\"].shape)\n",
        "\n",
        "train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_loader   = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True)\n",
        "\n",
        "COND_DIM = ds_train[0][\"cond\"].numel()\n",
        "print(\"COND_DIM:\", COND_DIM)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:04:00.991086Z",
          "iopub.execute_input": "2025-12-30T17:04:00.99134Z",
          "iopub.status.idle": "2025-12-30T17:04:01.04856Z",
          "shell.execute_reply.started": "2025-12-30T17:04:00.991314Z",
          "shell.execute_reply": "2025-12-30T17:04:01.047921Z"
        },
        "id": "kTOJvPVBVRhW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 4: Blocks + DWT (FIXED) + Encoders (OCT + Optional Fundus Stub)\n",
        "# =========================\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1, act=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, k, s, p, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True) if act else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.c1 = ConvBNAct(ch, ch, 3, 1, 1)\n",
        "        self.c2 = ConvBNAct(ch, ch, 3, 1, 1, act=False)\n",
        "    def forward(self, x):\n",
        "        return F.silu(x + self.c2(self.c1(x)))\n",
        "\n",
        "class HaarDWT(nn.Module):\n",
        "    \"\"\"\n",
        "    Correct grouped-conv Haar DWT\n",
        "    Input : (B,C,H,W)\n",
        "    Output: (B,4C,H/2,W/2) bands concatenated per-channel\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ll = torch.tensor([[0.5, 0.5],[0.5, 0.5]], dtype=torch.float32)\n",
        "        lh = torch.tensor([[0.5, 0.5],[-0.5,-0.5]], dtype=torch.float32)\n",
        "        hl = torch.tensor([[0.5,-0.5],[0.5,-0.5]], dtype=torch.float32)\n",
        "        hh = torch.tensor([[0.5,-0.5],[-0.5,0.5]], dtype=torch.float32)\n",
        "        filt = torch.stack([ll, lh, hl, hh], dim=0)  # (4,2,2)\n",
        "        self.register_buffer(\"filt\", filt)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        filt = self.filt.unsqueeze(1)                 # (4,1,2,2)\n",
        "        weight = filt.repeat(C, 1, 1, 1)              # (4C,1,2,2)\n",
        "        y = F.conv2d(x, weight, stride=2, padding=0, groups=C)  # (B,4C,H/2,W/2)\n",
        "        return y\n",
        "\n",
        "class OCTEncoder(nn.Module):\n",
        "    def __init__(self, in_ch=1, base=64, feat_ch=256):\n",
        "        super().__init__()\n",
        "        self.spatial = nn.Sequential(\n",
        "            ConvBNAct(in_ch, base, 5, 2, 2),      # 128\n",
        "            ConvBNAct(base, base, 3, 1, 1),\n",
        "            ConvBNAct(base, base*2, 3, 2, 1),     # 64\n",
        "            ResBlock(base*2),\n",
        "            ResBlock(base*2),\n",
        "            ConvBNAct(base*2, feat_ch, 1, 1, 0)\n",
        "        )\n",
        "        self.dwt = HaarDWT()\n",
        "        self.freq_proj = nn.Sequential(\n",
        "            ConvBNAct(feat_ch*4, feat_ch, 1, 1, 0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_sp = self.spatial(x)                         # (B,256,64,64)\n",
        "        f_dwt = self.dwt(f_sp)                         # (B,1024,32,32)\n",
        "        f_dwt = F.interpolate(f_dwt, size=f_sp.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "        f_fr = self.freq_proj(f_dwt)                   # (B,256,64,64)\n",
        "        return f_sp, f_fr\n",
        "\n",
        "class FundusEncoderStub(nn.Module):\n",
        "    \"\"\"\n",
        "    Kept for architectural compatibility (inactive if no fundus data).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, x):\n",
        "        raise RuntimeError(\"Fundus encoder is inactive in OCT-only training.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:04:05.132197Z",
          "iopub.execute_input": "2025-12-30T17:04:05.132908Z",
          "iopub.status.idle": "2025-12-30T17:04:05.143943Z",
          "shell.execute_reply.started": "2025-12-30T17:04:05.132886Z",
          "shell.execute_reply": "2025-12-30T17:04:05.143218Z"
        },
        "id": "XNapQe6AVRhX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 5: HFSF + Generator + Discriminators\n",
        "# =========================\n",
        "class HFSFBlock(nn.Module):\n",
        "    def __init__(self, ch=256, out_ch=512):\n",
        "        super().__init__()\n",
        "        self.gate = nn.Sequential(nn.Conv2d(ch, ch, 1, 1, 0), nn.Sigmoid())\n",
        "        self.freq_mix = nn.Sequential(\n",
        "            ConvBNAct(ch, ch, 1, 1, 0),\n",
        "            ConvBNAct(ch, ch, 3, 1, 1),\n",
        "        )\n",
        "        self.harm = nn.Sequential(\n",
        "            nn.Conv2d(ch, out_ch, 1, 1, 0, bias=False),\n",
        "            nn.GroupNorm(8, out_ch),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, 1, 1, groups=out_ch, bias=False),\n",
        "            nn.Conv2d(out_ch, out_ch, 1, 1, 0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, f_sp, f_fr):\n",
        "        g = self.gate(f_sp)\n",
        "        f_fft = fft_mag(f_sp)\n",
        "        f_freq = self.freq_mix(f_fr + f_fft)\n",
        "        fused = g * f_freq + (1.0 - g) * f_sp\n",
        "        return self.harm(fused)\n",
        "\n",
        "class MappingNet(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, out_dim),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(out_dim, out_dim),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "    def forward(self, p):\n",
        "        return self.net(p)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, fused_ch=512, z_dim=128, cond_dim=0, out_ch=1):\n",
        "        super().__init__()\n",
        "        self.use_cond = cond_dim > 0\n",
        "        self.map_p = MappingNet(cond_dim, 512) if self.use_cond else None\n",
        "        self.z_proj = nn.Linear(z_dim, 512)\n",
        "\n",
        "        self.dec1 = ConvBNAct(fused_ch + 512, 512, 3, 1, 1)\n",
        "        self.dec2 = ConvBNAct(512, 256, 3, 1, 1)\n",
        "\n",
        "        self.up1 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),  # 128\n",
        "            ConvBNAct(256, 128, 3, 1, 1),\n",
        "            ResBlock(128)\n",
        "        )\n",
        "        self.up2 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),  # 256\n",
        "            ConvBNAct(128, 64, 3, 1, 1),\n",
        "            ResBlock(64)\n",
        "        )\n",
        "        self.to_img = nn.Conv2d(64, out_ch, 1, 1, 0)\n",
        "\n",
        "    def forward(self, fused_feat, z, p=None):\n",
        "        B, _, H, W = fused_feat.shape\n",
        "        zc = self.z_proj(z)\n",
        "        style = zc\n",
        "        if self.use_cond and p is not None:\n",
        "            style = style + self.map_p(p)\n",
        "        style_map = style.view(B, 512, 1, 1).expand(B, 512, H, W)\n",
        "        x = torch.cat([fused_feat, style_map], dim=1)\n",
        "        x = self.dec2(self.dec1(x))\n",
        "        x = self.up1(x)\n",
        "        x = self.up2(x)\n",
        "        return torch.tanh(self.to_img(x))\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, in_ch=1, base=64):\n",
        "        super().__init__()\n",
        "        def block(ic, oc, s):\n",
        "            return nn.Sequential(\n",
        "                nn.utils.spectral_norm(nn.Conv2d(ic, oc, 4, s, 1)),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        self.net = nn.Sequential(\n",
        "            block(in_ch, base, 2),\n",
        "            block(base, base*2, 2),\n",
        "            block(base*2, base*4, 2),\n",
        "            block(base*4, base*8, 1),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(base*8, 1, 4, 1, 1))\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class FrequencyDiscriminator(nn.Module):\n",
        "    def __init__(self, in_ch=1, base=32):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(in_ch, base, 3, 2, 1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(base, base*2, 3, 2, 1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(base*2, base*4, 3, 2, 1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(base*4, 1, 3, 1, 1)),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(fft_mag(x))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:04:13.085764Z",
          "iopub.execute_input": "2025-12-30T17:04:13.086506Z",
          "iopub.status.idle": "2025-12-30T17:04:13.100849Z",
          "shell.execute_reply.started": "2025-12-30T17:04:13.086477Z",
          "shell.execute_reply": "2025-12-30T17:04:13.100171Z"
        },
        "id": "6_-fpX08VRhY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 6: Init Models + Training Loop (OCT-only Augmentation)\n",
        "# =========================\n",
        "enc  = OCTEncoder(in_ch=1).to(DEVICE)\n",
        "hfsf = HFSFBlock(ch=256, out_ch=512).to(DEVICE)\n",
        "gen  = Generator(fused_ch=512, z_dim=Z_DIM, cond_dim=COND_DIM, out_ch=1).to(DEVICE)\n",
        "\n",
        "d_anat = PatchDiscriminator(in_ch=1).to(DEVICE)\n",
        "d_freq = FrequencyDiscriminator(in_ch=1).to(DEVICE)\n",
        "\n",
        "opt_g = torch.optim.Adam(list(enc.parameters()) + list(hfsf.parameters()) + list(gen.parameters()),\n",
        "                         lr=LR_G, betas=(0.5, 0.999))\n",
        "opt_d = torch.optim.Adam(list(d_anat.parameters()) + list(d_freq.parameters()),\n",
        "                         lr=LR_D, betas=(0.5, 0.999))\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
        "\n",
        "fixed = next(iter(val_loader))\n",
        "fixed_real = fixed[\"img\"].to(DEVICE)\n",
        "fixed_cond = fixed[\"cond\"].to(DEVICE)\n",
        "fixed_z = torch.randn(fixed_real.size(0), Z_DIM, device=DEVICE)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    enc.train(); hfsf.train(); gen.train()\n",
        "    d_anat.train(); d_freq.train()\n",
        "\n",
        "    for step, batch in enumerate(train_loader, start=1):\n",
        "        real = batch[\"img\"].to(DEVICE)      # (B,1,256,256) [-1,1]\n",
        "        cond = batch[\"cond\"].to(DEVICE)     # (B,COND_DIM)\n",
        "\n",
        "        # ---- Train D ----\n",
        "        z = torch.randn(real.size(0), Z_DIM, device=DEVICE)\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
        "            f_sp, f_fr = enc(real)\n",
        "            fused = hfsf(f_sp, f_fr)\n",
        "            fake = gen(fused, z, cond).detach()\n",
        "\n",
        "            loss_d = hinge_d_loss(d_anat(real), d_anat(fake)) + hinge_d_loss(d_freq(real), d_freq(fake))\n",
        "\n",
        "        opt_d.zero_grad(set_to_none=True)\n",
        "        scaler.scale(loss_d).backward()\n",
        "        scaler.step(opt_d)\n",
        "\n",
        "        # ---- Train G (+ Encoder + HFSF) ----\n",
        "        z = torch.randn(real.size(0), Z_DIM, device=DEVICE)\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
        "            f_sp, f_fr = enc(real)\n",
        "            fused = hfsf(f_sp, f_fr)\n",
        "            fake = gen(fused, z, cond)\n",
        "\n",
        "            adv   = hinge_g_loss(d_anat(fake)) + hinge_g_loss(d_freq(fake))\n",
        "            l1    = F.l1_loss(fake, real)\n",
        "            lssim = ssim_loss(fake, real)\n",
        "            lfreq = F.l1_loss(fft_mag(fake), fft_mag(real))\n",
        "\n",
        "            loss_g = adv + (LAMBDA_L1 * l1) + (LAMBDA_SSIM * lssim) + (LAMBDA_FREQ * lfreq)\n",
        "\n",
        "        opt_g.zero_grad(set_to_none=True)\n",
        "        scaler.scale(loss_g).backward()\n",
        "        scaler.step(opt_g)\n",
        "        scaler.update()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Epoch {epoch}/{EPOCHS} Step {step} | D {loss_d.item():.4f} | G {loss_g.item():.4f} | adv {adv.item():.4f}\")\n",
        "\n",
        "    # Save samples each epoch\n",
        "    enc.eval(); hfsf.eval(); gen.eval()\n",
        "    with torch.no_grad():\n",
        "        f_sp, f_fr = enc(fixed_real)\n",
        "        fused = hfsf(f_sp, f_fr)\n",
        "        fake = gen(fused, fixed_z, fixed_cond)\n",
        "\n",
        "        grid = torch.cat([fixed_real, fake], dim=0)\n",
        "        save_image((grid + 1) / 2, os.path.join(OUT_DIR, f\"samples_epoch_{epoch}.png\"), nrow=fixed_real.size(0))\n",
        "\n",
        "    torch.save({\n",
        "        \"enc\": enc.state_dict(),\n",
        "        \"hfsf\": hfsf.state_dict(),\n",
        "        \"gen\": gen.state_dict(),\n",
        "        \"d_anat\": d_anat.state_dict(),\n",
        "        \"d_freq\": d_freq.state_dict(),\n",
        "        \"epoch\": epoch,\n",
        "    }, os.path.join(OUT_DIR, f\"ckpt_epoch_{epoch}.pt\"))\n",
        "\n",
        "    print(\"Saved:\", f\"samples_epoch_{epoch}.png\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:04:18.668979Z",
          "iopub.execute_input": "2025-12-30T17:04:18.66965Z",
          "iopub.status.idle": "2025-12-30T17:14:09.225074Z",
          "shell.execute_reply.started": "2025-12-30T17:04:18.669622Z",
          "shell.execute_reply": "2025-12-30T17:14:09.22433Z"
        },
        "id": "rbyJGyA9VRhY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # =========================\n",
        "# # Cell 7 (FINAL): Load ckpt_epoch_18 + Augmentation Generation\n",
        "# # =========================\n",
        "# import os, glob\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# import tifffile as tiff\n",
        "\n",
        "# # --------- 1) Load Checkpoint (Epoch 18) ----------\n",
        "# CKPT_PATH = \"/kaggle/working/hmdf_outputs/ckpt_epoch_18.pt\"\n",
        "\n",
        "# ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "# enc.load_state_dict(ckpt[\"enc\"])\n",
        "# hfsf.load_state_dict(ckpt[\"hfsf\"])\n",
        "# gen.load_state_dict(ckpt[\"gen\"])\n",
        "\n",
        "# enc.eval(); hfsf.eval(); gen.eval()\n",
        "# print(\"Loaded checkpoint:\", CKPT_PATH)\n",
        "\n",
        "# # --------- 2) Output folder ----------\n",
        "# AUG_DIR = \"/kaggle/working/augmented_octs_epoch18\"\n",
        "# os.makedirs(AUG_DIR, exist_ok=True)\n",
        "\n",
        "# # --------- 3) Augmentation settings ----------\n",
        "# N_SYN_PER_REAL = 5\n",
        "# MAX_REAL = 2   # set 2 if you want only 2 images demo\n",
        "# SOURCE_DIR = TRAIN_DIR  # or VAL_DIR / TEST_DIR\n",
        "\n",
        "# real_paths = sorted(glob.glob(os.path.join(SOURCE_DIR, \"*.tiff\")))[:MAX_REAL]\n",
        "# print(\"Total real images selected:\", len(real_paths))\n",
        "\n",
        "# def save_tiff_from_tensor(x01: torch.Tensor, path: str):\n",
        "#     # x01: (H,W) in [0,1]\n",
        "#     arr = (x01.clamp(0,1).cpu().numpy() * 255).astype(np.uint8)\n",
        "#     tiff.imwrite(path, arr)\n",
        "\n",
        "# # --------- 4) Generate synthetic images ----------\n",
        "# for pth in real_paths:\n",
        "#     pid, tp, orient = parse_filename(pth)\n",
        "\n",
        "#     # Load TIFF exactly like dataset\n",
        "#     arr = tiff.imread(pth)\n",
        "#     if arr.ndim == 3:\n",
        "#         arr = arr[..., 0]\n",
        "#     arr = arr.astype(np.float32)\n",
        "#     arr = (arr - arr.min()) / ((arr.max() - arr.min()) + 1e-8)\n",
        "\n",
        "#     x = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
        "#     x = F.interpolate(x, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "#     x = x.squeeze(0)                                     # (1,256,256)\n",
        "#     x = x * 2 - 1                                        # [-1,1]\n",
        "#     xB = x.unsqueeze(0).to(DEVICE)                       # (1,1,256,256)\n",
        "\n",
        "#     # Condition vector\n",
        "#     tp_vec = torch.zeros(len(TIMEPOINTS), dtype=torch.float32)\n",
        "#     if tp in TIMEPOINTS:\n",
        "#         tp_vec[TIMEPOINTS.index(tp)] = 1.0\n",
        "\n",
        "#     if ds_train.meta is not None and pid in ds_train.meta.index:\n",
        "#         row = ds_train.meta.loc[pid]\n",
        "#         vals = []\n",
        "#         for c in ds_train.cond_cols:\n",
        "#             v = row[c]\n",
        "#             if pd.isna(v):\n",
        "#                 v = 0.0\n",
        "#             vals.append(float(v))\n",
        "#         cvec = torch.tensor(vals, dtype=torch.float32)\n",
        "#         cvec = (cvec - cvec.mean()) / (cvec.std() + 1e-6)\n",
        "#         cond = torch.cat([cvec, tp_vec], dim=0)\n",
        "#     else:\n",
        "#         cond = tp_vec\n",
        "\n",
        "#     cond = cond.unsqueeze(0).to(DEVICE)  # (1,COND_DIM)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         f_sp, f_fr = enc(xB)\n",
        "#         fused = hfsf(f_sp, f_fr)\n",
        "\n",
        "#         for k in range(N_SYN_PER_REAL):\n",
        "#             z = torch.randn(1, Z_DIM, device=DEVICE)\n",
        "#             fake = gen(fused, z, cond)  # (1,1,256,256) in [-1,1]\n",
        "#             fake01 = (fake.squeeze(0).squeeze(0) + 1) / 2  # (256,256) in [0,1]\n",
        "\n",
        "#             out_name = f\"{pid}_{tp}_{orient}_syn{k+1}_E18.tiff\"\n",
        "#             out_path = os.path.join(AUG_DIR, out_name)\n",
        "#             save_tiff_from_tensor(fake01, out_path)\n",
        "\n",
        "# print(\"Saved augmented TIFFs in:\", AUG_DIR)\n",
        "# print(\"Count:\", len(os.listdir(AUG_DIR)))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "e2IPJdgXVRhZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 7 (FINAL): Augmentation from 2 TEST images (Patient 104)\n",
        "# =========================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tifffile as tiff\n",
        "\n",
        "# --------- Load checkpoint ----------\n",
        "CKPT_PATH = \"/kaggle/working/hmdf_outputs/ckpt_epoch_18.pt\"\n",
        "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "\n",
        "enc.load_state_dict(ckpt[\"enc\"])\n",
        "hfsf.load_state_dict(ckpt[\"hfsf\"])\n",
        "gen.load_state_dict(ckpt[\"gen\"])\n",
        "\n",
        "enc.eval(); hfsf.eval(); gen.eval()\n",
        "print(\"Loaded checkpoint:\", CKPT_PATH)\n",
        "\n",
        "# --------- Output folder ----------\n",
        "AUG_DIR = \"/kaggle/working/augmented_img_final\"\n",
        "os.makedirs(AUG_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "real_paths = sorted(glob.glob(\"/kaggle/input/oct-postsurgery-visual-improvement/oct_data/test/octs/*.tiff\"))[:98]\n",
        "\n",
        "N_SYN_PER_REAL = 1\n",
        "\n",
        "def save_tiff_from_tensor(x01, path):\n",
        "    arr = (x01.clamp(0,1).cpu().numpy() * 255).astype(np.uint8)\n",
        "    tiff.imwrite(path, arr)\n",
        "\n",
        "# --------- Augmentation loop ----------\n",
        "for pth in real_paths:\n",
        "    pid, tp, orient = parse_filename(pth)\n",
        "\n",
        "    arr = tiff.imread(pth)\n",
        "    if arr.ndim == 3:\n",
        "        arr = arr[..., 0]\n",
        "\n",
        "    arr = arr.astype(np.float32)\n",
        "    arr = (arr - arr.min()) / ((arr.max() - arr.min()) + 1e-8)\n",
        "\n",
        "    x = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0)\n",
        "    x = F.interpolate(x, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "    x = x.squeeze(0)\n",
        "    x = x * 2 - 1\n",
        "    xB = x.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # ---- condition vector ----\n",
        "    tp_vec = torch.zeros(len(TIMEPOINTS))\n",
        "    if tp in TIMEPOINTS:\n",
        "        tp_vec[TIMEPOINTS.index(tp)] = 1.0\n",
        "\n",
        "    if ds_train.meta is not None and pid in ds_train.meta.index:\n",
        "        row = ds_train.meta.loc[pid]\n",
        "        vals = []\n",
        "        for c in ds_train.cond_cols:\n",
        "            v = row[c]\n",
        "            if pd.isna(v): v = 0.0\n",
        "            vals.append(float(v))\n",
        "        cvec = torch.tensor(vals)\n",
        "        cvec = (cvec - cvec.mean()) / (cvec.std() + 1e-6)\n",
        "        cond = torch.cat([cvec, tp_vec], dim=0)\n",
        "    else:\n",
        "        cond = tp_vec\n",
        "\n",
        "    cond = cond.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        f_sp, f_fr = enc(xB)\n",
        "        fused = hfsf(f_sp, f_fr)\n",
        "\n",
        "        for k in range(N_SYN_PER_REAL):\n",
        "            z = torch.randn(1, Z_DIM, device=DEVICE)\n",
        "            fake = gen(fused, z, cond)\n",
        "            fake01 = (fake.squeeze() + 1) / 2\n",
        "\n",
        "            out_name = f\"{pid}_{tp}_{orient}_syn{k+1}_E18.tiff\"\n",
        "            save_tiff_from_tensor(fake01, os.path.join(AUG_DIR, out_name))\n",
        "\n",
        "print(\"Augmented images saved in:\", AUG_DIR)\n",
        "print(\"Total:\", len(os.listdir(AUG_DIR)))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T18:02:01.708226Z",
          "iopub.execute_input": "2025-12-30T18:02:01.708859Z",
          "iopub.status.idle": "2025-12-30T18:02:05.689709Z",
          "shell.execute_reply.started": "2025-12-30T18:02:01.708836Z",
          "shell.execute_reply": "2025-12-30T18:02:05.689093Z"
        },
        "id": "hnkzrH1bVRhZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 8: Side-by-Side Comparison (Original vs Synthetic)\n",
        "# =========================\n",
        "import matplotlib.pyplot as plt\n",
        "import tifffile as tiff\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# -------- Paths --------\n",
        "ORIGINAL_PATH = \"/kaggle/input/oct-postsurgery-visual-improvement/oct_data/test/octs/104_2weeks_H.tiff\"\n",
        "SYN_DIR = \"/kaggle/working/augmented_patient_104\"\n",
        "\n",
        "# Select synthetic images to compare\n",
        "SYN_FILES = [\n",
        "    \"104_2weeks_H_syn1_E18.tiff\",\n",
        "    \"104_2weeks_H_syn2_E18.tiff\",\n",
        "    \"104_2weeks_H_syn3_E18.tiff\",\n",
        "    \"104_2weeks_H_syn6_E18.tiff\",\n",
        "    \"104_2weeks_H_syn10_E18.tiff\",\n",
        "]\n",
        "\n",
        "# -------- Load images --------\n",
        "orig = tiff.imread(ORIGINAL_PATH)\n",
        "if orig.ndim == 3:\n",
        "    orig = orig[..., 0]\n",
        "\n",
        "syn_images = []\n",
        "for f in SYN_FILES:\n",
        "    p = os.path.join(SYN_DIR, f)\n",
        "    img = tiff.imread(p)\n",
        "    syn_images.append(img)\n",
        "\n",
        "# -------- Plot --------\n",
        "n_cols = len(syn_images) + 1\n",
        "plt.figure(figsize=(3 * n_cols, 4))\n",
        "\n",
        "# Original\n",
        "plt.subplot(1, n_cols, 1)\n",
        "plt.imshow(orig, cmap=\"gray\")\n",
        "plt.title(\"Original OCT\\n(104 | 2 weeks | H)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Synthetic\n",
        "for i, img in enumerate(syn_images):\n",
        "    plt.subplot(1, n_cols, i + 2)\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    plt.title(f\"Synthetic {i+1}\\n(Epoch 18)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Side-by-Side Comparison: Original vs Synthetic OCT Images\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:20:50.623105Z",
          "iopub.execute_input": "2025-12-30T17:20:50.623676Z",
          "iopub.status.idle": "2025-12-30T17:20:51.182724Z",
          "shell.execute_reply.started": "2025-12-30T17:20:50.62365Z",
          "shell.execute_reply": "2025-12-30T17:20:51.181993Z"
        },
        "id": "_-MFG2ihVRha"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "REAL_DIR = \"/kaggle/input/oct-postsurgery-visual-improvement/oct_data/test/octs\"\n",
        "SYN_DIR  = \"/kaggle/working/augmented_patient_104\"\n",
        "\n",
        "ssim_scores, psnr_scores = [], []\n",
        "\n",
        "real_imgs = sorted(glob.glob(os.path.join(REAL_DIR, \"*.tiff\")))\n",
        "syn_imgs  = sorted(glob.glob(os.path.join(SYN_DIR, \"*.tiff\")))\n",
        "\n",
        "for r, s in zip(real_imgs, syn_imgs):\n",
        "    real = cv2.imread(r, cv2.IMREAD_GRAYSCALE)\n",
        "    syn  = cv2.imread(s, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    real = cv2.resize(real, (256,256))\n",
        "    syn  = cv2.resize(syn, (256,256))\n",
        "\n",
        "    ssim_scores.append(ssim(real, syn, data_range=255))\n",
        "    psnr_scores.append(psnr(real, syn, data_range=255))\n",
        "\n",
        "print(\"SSIM mean:\", np.mean(ssim_scores))\n",
        "print(\"PSNR mean:\", np.mean(psnr_scores))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T17:41:39.00451Z",
          "iopub.execute_input": "2025-12-30T17:41:39.005069Z",
          "iopub.status.idle": "2025-12-30T17:41:39.483428Z",
          "shell.execute_reply.started": "2025-12-30T17:41:39.005045Z",
          "shell.execute_reply": "2025-12-30T17:41:39.482781Z"
        },
        "id": "ohpCNGPHVRha"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import numpy as np\n",
        "import tifffile as tiff\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "from scipy.linalg import sqrtm\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "REAL_DIR = \"/kaggle/input/oct-postsurgery-visual-improvement/oct_data/test/octs\"\n",
        "SYN_DIR  = \"/kaggle/working/augmented_img_final\"\n",
        "\n",
        "# ---- choose real images ----\n",
        "# Option A: only patient 104 (recommended for consistency)\n",
        "real_paths = sorted(glob.glob(os.path.join(REAL_DIR, \"*.tiff\")))\n",
        "\n",
        "# Option B: If you want more real images for stable FID, use more patients:\n",
        "# real_paths = sorted(glob.glob(os.path.join(REAL_DIR, \"*.tiff\")))[:50]\n",
        "\n",
        "syn_paths  = sorted(glob.glob(os.path.join(SYN_DIR, \"*.tiff\")))\n",
        "\n",
        "print(\"Real images:\", len(real_paths))\n",
        "print(\"Synthetic images:\", len(syn_paths))\n",
        "\n",
        "assert len(real_paths) > 0, \"No real images found!\"\n",
        "assert len(syn_paths)  > 0, \"No synthetic images found!\"\n",
        "\n",
        "# ---- Load and preprocess for Inception (299x299, RGB) ----\n",
        "def load_for_inception(paths, max_images=50):\n",
        "    imgs = []\n",
        "    for p in paths[:max_images]:\n",
        "        img = tiff.imread(p)\n",
        "        if img.ndim == 3:\n",
        "            img = img[..., 0]\n",
        "        img = img.astype(np.float32)\n",
        "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)  # [0,1]\n",
        "        img = torch.from_numpy(img).unsqueeze(0)  # (1,H,W)\n",
        "\n",
        "        img = img.unsqueeze(0)  # (1,1,H,W)\n",
        "        img = F.interpolate(img, size=(299, 299), mode=\"bilinear\", align_corners=False)\n",
        "        img = img.squeeze(0)    # (1,299,299)\n",
        "\n",
        "        img = img.repeat(3, 1, 1)  # (3,299,299) grayscale -> RGB\n",
        "        imgs.append(img)\n",
        "    return torch.stack(imgs, dim=0)  # (N,3,299,299)\n",
        "\n",
        "# ---- Inception feature extractor ----\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "\n",
        "weights = Inception_V3_Weights.DEFAULT\n",
        "inception = inception_v3(weights=weights)   # aux_logits default = True\n",
        "inception.fc = torch.nn.Identity()\n",
        "inception = inception.to(DEVICE).eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_feats(batch_imgs, batch_size=8):\n",
        "    feats = []\n",
        "    for i in range(0, batch_imgs.size(0), batch_size):\n",
        "        x = batch_imgs[i:i+batch_size].to(DEVICE)\n",
        "        f = inception(x)\n",
        "        feats.append(f.cpu().numpy())\n",
        "    return np.concatenate(feats, axis=0)\n",
        "\n",
        "# ---- FID computation ----\n",
        "def fid_score(real_feats, gen_feats):\n",
        "    mu_r, mu_g = real_feats.mean(axis=0), gen_feats.mean(axis=0)\n",
        "    sig_r = np.cov(real_feats, rowvar=False)\n",
        "    sig_g = np.cov(gen_feats, rowvar=False)\n",
        "\n",
        "    covmean = sqrtm(sig_r @ sig_g)\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    diff = mu_r - mu_g\n",
        "    fid = diff @ diff + np.trace(sig_r + sig_g - 2 * covmean)\n",
        "    return float(fid)\n",
        "\n",
        "# ---- Run ----\n",
        "real_imgs = load_for_inception(real_paths, max_images=50)\n",
        "syn_imgs  = load_for_inception(syn_paths,  max_images=50)\n",
        "\n",
        "real_feats = get_feats(real_imgs)\n",
        "syn_feats  = get_feats(syn_imgs)\n",
        "\n",
        "fid = fid_score(real_feats, syn_feats)\n",
        "print(\" FID:\", fid)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-30T18:05:08.516478Z",
          "iopub.execute_input": "2025-12-30T18:05:08.51677Z",
          "iopub.status.idle": "2025-12-30T18:05:16.01868Z",
          "shell.execute_reply.started": "2025-12-30T18:05:08.516748Z",
          "shell.execute_reply": "2025-12-30T18:05:16.017852Z"
        },
        "id": "v-XERjdKVRha"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}